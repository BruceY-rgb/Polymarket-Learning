import os
import json
import pandas as pd
from gql import gql, Client
from gql.transport.requests import RequestsHTTPTransport
from flatten_json import flatten
from datetime import datetime, timedelta, timezone
import subprocess
import time
from update_utils.update_markets import update_markets

# Global runtime timestamp - set once when program starts
RUNTIME_TIMESTAMP = datetime.now().strftime('%Y%m%d_%H%M%S')

# é»˜è®¤æ—¶é—´èŒƒå›´é™åˆ¶ï¼ˆè®¾ç½®ä¸ºåŠå¹´å›æº¯ï¼‰
DEFAULT_DAYS_LIMIT = 180  # åŠå¹´ = 180 å¤©

# Columns to save
COLUMNS_TO_SAVE = ['timestamp', 'maker', 'makerAssetId', 'makerAmountFilled', 'taker', 'takerAssetId', 'takerAmountFilled', 'transactionHash']

# No need to create goldsky directory anymore - files go to root directory

CURSOR_FILE = '/Users/yangsmac/Desktop/poly_data/cursor_state.json'

def save_cursor(timestamp, last_id, sticky_timestamp=None):
    """Save cursor state to file for efficient resume."""
    state = {
        'last_timestamp': timestamp,
        'last_id': last_id,
        'sticky_timestamp': sticky_timestamp
    }
    with open(CURSOR_FILE, 'w') as f:
        json.dump(state, f)

def get_latest_cursor(days_limit: int = DEFAULT_DAYS_LIMIT):
    """è·å–æœ€æ–°çš„å…‰æ ‡çŠ¶æ€ä»¥é«˜æ•ˆæ¢å¤ã€‚
    è¿”å› (timestamp, last_id, sticky_timestamp) å…ƒç»„ã€‚
    æ–°ç­–ç•¥ï¼šä»æœ€æ–°æ•°æ®å¼€å§‹ï¼Œå‘å‰å›æº¯æŒ‡å®šå¤©æ•°
    Args:
        days_limit: æ—¶é—´èŒƒå›´é™åˆ¶ï¼ˆå¤©æ•°ï¼‰ï¼Œé»˜è®¤å‘å‰å›æº¯ 180 å¤©ï¼ˆåŠå¹´ï¼‰
    """
    # è®¡ç®—æ—¶é—´èŒƒå›´
    # ä½¿ç”¨æ›´å…¼å®¹çš„æ—¶åŒºå¤„ç†æ–¹å¼
    current_time = datetime.now(tz=timezone.utc)
    start_time = current_time - timedelta(days=days_limit)
    start_timestamp = int(start_time.timestamp())
    current_timestamp = int(current_time.timestamp())

    print(f"ğŸ”„ æ•°æ®æ”¶é›†ç­–ç•¥ï¼šä»æœ€æ–°æ•°æ®å¼€å§‹ï¼Œå‘å‰å›æº¯ {days_limit} å¤©")
    print(f"ğŸ“… èµ·å§‹æ—¶é—´ï¼ˆå›æº¯èµ·ç‚¹ï¼‰: {start_time.strftime('%Y-%m-%d %H:%M:%S UTC')} (timestamp: {start_timestamp})")
    print(f"ğŸ“… å½“å‰æ—¶é—´: {current_time.strftime('%Y-%m-%d %H:%M:%S UTC')} (timestamp: {current_timestamp})")

    # Fallback: read from CSV file
    cache_file = '/Users/yangsmac/Desktop/poly_data/orderFilled.csv'

    # æ£€æŸ¥æ˜¯å¦å·²æœ‰æ•°æ®æ–‡ä»¶
    if os.path.isfile(cache_file):
        try:
            # è·å–æ–‡ä»¶ä¸­çš„æœ€æ–°æ—¶é—´æˆ³
            result = subprocess.run(['tail', '-n', '1', cache_file], capture_output=True, text=True, check=True)
            last_line = result.stdout.strip()
            if last_line:
                # Get header to find column indices
                header_result = subprocess.run(['head', '-n', '1', cache_file], capture_output=True, text=True, check=True)
                headers = header_result.stdout.strip().split(',')

                if 'timestamp' in headers:
                    timestamp_index = headers.index('timestamp')
                    values = last_line.split(',')
                    if len(values) > timestamp_index:
                        file_last_timestamp = int(values[timestamp_index])

                        # å¦‚æœæ–‡ä»¶ä¸­çš„æœ€æ–°æ•°æ®æ™šäºæˆ‘ä»¬çš„å›æº¯èµ·ç‚¹ï¼Œä»æœ€æ–°å¼€å§‹
                        if file_last_timestamp > start_timestamp:
                            readable_time = datetime.fromtimestamp(file_last_timestamp, tz=timezone.utc).strftime('%Y-%m-%d %H:%M:%S UTC')
                            print(f"âœ… å‘ç°è¾ƒæ–°æ•°æ®ï¼šæ–‡ä»¶æœ€æ–° timestamp {file_last_timestamp} ({readable_time})")
                            print(f"ğŸ”„ ä»æœ€æ–°æ•°æ®å¼€å§‹å›æº¯...")
                            # ä»æ–‡ä»¶ä¸­çš„æœ€æ–°æ—¶é—´æˆ³å¼€å§‹ï¼ˆå‡å»1ç§’ç¡®ä¿ä¸é‡å¤ï¼‰
                            return file_last_timestamp - 1, None, None
                        else:
                            print(f"âš ï¸ æ–‡ä»¶æ•°æ®è¾ƒæ—§ï¼šæœ€æ–° timestamp {file_last_timestamp} æ—©äºå›æº¯èµ·ç‚¹ {start_timestamp}")
                            print(f"ğŸ”„ é‡æ–°ä»å›æº¯èµ·ç‚¹å¼€å§‹...")
        except Exception as e:
            print(f"âš ï¸ è¯»å–æ–‡ä»¶å¤±è´¥: {e}")

    # å¦‚æœæ²¡æœ‰ç°æœ‰æ•°æ®æˆ–éœ€è¦é‡æ–°å¼€å§‹ï¼Œä»å½“å‰æ—¶é—´å¼€å§‹å›æº¯
    print(f"ğŸš€ åˆå§‹è¿è¡Œï¼šä»å½“å‰æ—¶é—´å¼€å§‹å›æº¯ {days_limit} å¤©")
    print(f"â° èµ·å§‹ç‚¹: {current_time.strftime('%Y-%m-%d %H:%M:%S UTC')} (timestamp: {current_timestamp})")
    return current_timestamp, None, None

def scrape(at_once=1000, days_limit: int = DEFAULT_DAYS_LIMIT):
    """ä»æœ€æ–°æ•°æ®å¼€å§‹æŠ“å–è®¢å•æˆäº¤äº‹ä»¶ï¼Œå‘å‰å›æº¯æŒ‡å®šå¤©æ•°"""
    QUERY_URL = "https://api.goldsky.com/api/public/project_cl6mb8i9h0003e201j6li0diw/subgraphs/orderbook-subgraph/0.0.1/gn"
    print(f"GraphQL ç«¯ç‚¹: {QUERY_URL}")
    print(f"è¿è¡Œæ—¶é—´æˆ³: {RUNTIME_TIMESTAMP}")

    # è®¡ç®—å›æº¯æ—¶é—´èŒƒå›´
    current_time = datetime.now(tz=timezone.utc)
    start_time = current_time - timedelta(days=days_limit)
    start_timestamp = int(start_time.timestamp())

    print(f"\nğŸ”„ ä»æœ€æ–°æ•°æ®å¼€å§‹ï¼Œå‘å‰å›æº¯ {days_limit} å¤©")
    print(f"ğŸ“… å›æº¯èµ·ç‚¹: {start_time.strftime('%Y-%m-%d %H:%M:%S UTC')} (timestamp: {start_timestamp})")

    # Get starting cursor from latest file (includes sticky state for perfect resume)
    last_timestamp, last_id, sticky_timestamp = get_latest_cursor(days_limit)
    count = 0
    total_records = 0

    print(f"\nğŸš€ å¼€å§‹æŠ“å– orderFilledEvents")
    print(f"ğŸ“‚ è¾“å‡ºæ–‡ä»¶: /Users/yangsmac/Desktop/poly_data/orderFilled.csv")
    print(f"ğŸ“‹ ä¿å­˜åˆ—: {COLUMNS_TO_SAVE}")

    # å­˜å‚¨æ‰€æœ‰æ•°æ®ï¼Œæœ€åç»Ÿä¸€æ’åº
    all_data = []

    while True:
        # Build the where clause based on cursor state
        if sticky_timestamp is not None:
            # We're in sticky mode: stay at this timestamp and paginate by id
            where_clause = f'timestamp: "{sticky_timestamp}", id_lt: "{last_id}"'
        else:
            # å›æº¯æ¨¡å¼ï¼šä»å½“å‰æ—¶é—´æˆ³å‘å‰æŸ¥æ‰¾åˆ°æŒ‡å®šæ—¶é—´èŒƒå›´
            if last_timestamp is None:
                # ç¬¬ä¸€æ¬¡ï¼šä»å½“å‰æ—¶é—´å¼€å§‹
                where_clause = f'timestamp_lte: "{int(current_time.timestamp())}"'
            else:
                # ç»§ç»­å‘å‰å›æº¯
                where_clause = f'timestamp_lt: "{last_timestamp}", timestamp_gte: "{start_timestamp}"'

        q_string = f'''query MyQuery {{
                        orderFilledEvents(orderBy: timestamp, orderDirection: desc
                                             first: {at_once}
                                             where: {{{where_clause}}}) {{
                            fee
                            id
                            maker
                            makerAmountFilled
                            makerAssetId
                            orderHash
                            taker
                            takerAmountFilled
                            takerAssetId
                            timestamp
                            transactionHash
                        }}
                    }}'''

        query = gql(q_string)
        transport = RequestsHTTPTransport(url=QUERY_URL, verify=True, retries=3)
        client = Client(transport=transport)

        try:
            print(f"â³ è·å–æ‰¹æ¬¡ {count + 1}...")
            res = client.execute(query)
        except Exception as e:
            print(f"âŒ æŸ¥è¯¢é”™è¯¯: {e}")
            print("ğŸ”„ 5 ç§’åé‡è¯•...")
            time.sleep(5)
            continue

        if not res['orderFilledEvents'] or len(res['orderFilledEvents']) == 0:
            if sticky_timestamp is not None:
                # Exhausted events at sticky timestamp, advance to next timestamp
                last_timestamp = sticky_timestamp
                sticky_timestamp = None
                last_id = None
                continue
            print(f"âœ… æ²¡æœ‰æ›´å¤šæ•°æ®ï¼Œåœæ­¢æŠ“å–")
            break

        df = pd.DataFrame([flatten(x) for x in res['orderFilledEvents']]).reset_index(drop=True)

        # æ£€æŸ¥æ˜¯å¦åˆ°è¾¾å›æº¯æ—¶é—´èŒƒå›´
        batch_first_timestamp = int(df.iloc[-1]['timestamp'])  # æ³¨æ„ï¼šdesc æ’åºï¼Œæœ€åä¸€è¡Œæ˜¯æœ€æ—©çš„

        if batch_first_timestamp < start_timestamp:
            # æ•°æ®å·²ç»æ—©äºå›æº¯èŒƒå›´ï¼Œåœæ­¢
            print(f"ğŸ›‘ å·²åˆ°è¾¾å›æº¯æ—¶é—´èŒƒå›´è¾¹ç•Œ (timestamp: {batch_first_timestamp} < {start_timestamp})")
            # åªä¿ç•™åœ¨æ—¶é—´èŒƒå›´å†…çš„æ•°æ®
            df = df[df['timestamp'].astype(int) >= start_timestamp]
            if len(df) > 0:
                all_data.append(df)
                total_records += len(df)
                print(f"ğŸ“ æ·»åŠ æœ€åä¸€æ‰¹æ•°æ®ï¼š{len(df)} æ¡è®°å½•")
            break

        # Sort by timestamp and id for consistent ordering
        df = df.sort_values(['timestamp', 'id'], ascending=True).reset_index(drop=True)

        batch_last_timestamp = int(df.iloc[-1]['timestamp'])
        batch_last_id = df.iloc[-1]['id']
        batch_first_timestamp = int(df.iloc[0]['timestamp'])

        readable_time = datetime.fromtimestamp(batch_first_timestamp, tz=timezone.utc).strftime('%Y-%m-%d %H:%M:%S UTC')

        # Determine if we need sticky cursor for next iteration
        if len(df) >= at_once:
            # Batch is full - check if all events are at the same timestamp
            if batch_first_timestamp == batch_last_timestamp:
                # All events at same timestamp, need to continue paginating at this timestamp
                sticky_timestamp = batch_first_timestamp
                last_id = batch_last_id
                print(f"æ‰¹æ¬¡ {count + 1}: æ—¶é—´æˆ³ {batch_first_timestamp} ({readable_time}), è®°å½•æ•°: {len(df)} [STICKY - åŒæ—¶é—´æˆ³ç»§ç»­]")
            else:
                # Mixed timestamps - we need to continue from the earliest timestamp in this batch
                sticky_timestamp = batch_first_timestamp
                last_id = batch_last_id
                print(f"æ‰¹æ¬¡ {count + 1}: æ—¶é—´èŒƒå›´ {batch_first_timestamp}-{batch_last_timestamp} ({readable_time}), è®°å½•æ•°: {len(df)} [STICKY - ç¡®ä¿å®Œæ•´æ€§]")
        else:
            # Batch not full - we have all events, can advance normally
            if sticky_timestamp is not None:
                # We were in sticky mode, now exhausted - advance past this timestamp
                last_timestamp = sticky_timestamp
                sticky_timestamp = None
                last_id = None
                print(f"æ‰¹æ¬¡ {count + 1}: æ—¶é—´æˆ³ {batch_first_timestamp} ({readable_time}), è®°å½•æ•°: {len(df)} [STICKY å®Œæˆ]")
            else:
                # Normal backward traversal
                last_timestamp = batch_first_timestamp
                print(f"æ‰¹æ¬¡ {count + 1}: æœ€æ—©æ—¶é—´æˆ³ {batch_first_timestamp} ({readable_time}), è®°å½•æ•°: {len(df)}")

        count += 1
        all_data.append(df)

        # Fixed stop logic: only stop when we've truly reached the end
        # Check if we should stop based on time boundary
        if batch_first_timestamp < start_timestamp:
            # We've gone past the time range - this is already handled above
            # but keeping as safety check
            print("ğŸ›‘ å·²åˆ°è¾¾æ—¶é—´èŒƒå›´è¾¹ç•Œï¼Œåœæ­¢æŠ“å–")
            break
        elif len(df) < at_once and sticky_timestamp is None:
            # Batch not full AND not in sticky mode
            # This could mean we're at the end OR just reached a sparse period
            # We need to check if we've truly exhausted all data
            print(f"âš ï¸ æ‰¹æ¬¡ä¸æ»¡({len(df)}/{at_once})ï¼Œæ£€æŸ¥æ˜¯å¦åˆ°è¾¾è¾¹ç•Œ...")
            # Continue to next iteration to see if we get more data
            # If next iteration returns empty, then we know we're at the end

    # åˆå¹¶æ‰€æœ‰æ•°æ®å¹¶æ’åº
    if all_data:
        print(f"\nğŸ“Š åˆå¹¶æ•°æ®...")
        combined_df = pd.concat(all_data, ignore_index=True)
        # Remove duplicates (by id to be safe)
        combined_df = combined_df.drop_duplicates(subset=['id'])
        # æœ€ç»ˆæŒ‰æ—¶é—´æˆ³å‡åºæ’åº
        combined_df = combined_df.sort_values('timestamp', ascending=True).reset_index(drop=True)

        output_file = '/Users/yangsmac/Desktop/poly_data/orderFilled.csv'

        # ä¿å­˜æ•°æ®
        if os.path.isfile(output_file):
            # å¦‚æœæ–‡ä»¶å­˜åœ¨ï¼Œå…ˆè¯»å–ç°æœ‰æ•°æ®ï¼Œå»é‡ååˆå¹¶
            existing_df = pd.read_csv(output_file)
            if len(existing_df) > 0:
                # åˆå¹¶å¹¶å»é‡
                final_df = pd.concat([existing_df, combined_df]).drop_duplicates(subset=['id'])
                final_df = final_df.sort_values('timestamp', ascending=True).reset_index(drop=True)
            else:
                final_df = combined_df

            final_df.to_csv(output_file, index=None)
            total_records = len(final_df)
            print(f"âœ… æ›´æ–°æ–‡ä»¶ï¼š{output_file}")
        else:
            combined_df.to_csv(output_file, index=None)
            total_records = len(combined_df)
            print(f"âœ… åˆ›å»ºæ–‡ä»¶ï¼š{output_file}")

        print(f"ğŸ“ˆ æ€»è®°å½•æ•°ï¼š{total_records:,}")
    else:
        print("âš ï¸ æ²¡æœ‰è·å–åˆ°ä»»ä½•æ•°æ®")

    # Clear cursor file on successful completion
    if os.path.isfile(CURSOR_FILE):
        os.remove(CURSOR_FILE)

    print(f"\nğŸ‰ æŠ“å–å®Œæˆï¼")
    print(f"ğŸ“Š æ€»æ–°è®°å½•æ•°: {total_records}")
    print(f"ğŸ“ è¾“å‡ºæ–‡ä»¶: /Users/yangsmac/Desktop/poly_data/orderFilled.csv")

def update_goldsky(days_limit: int = DEFAULT_DAYS_LIMIT):
    """è¿è¡Œè®¢å•æˆäº¤äº‹ä»¶æŠ“å– - ä»æœ€æ–°æ•°æ®å¼€å§‹ï¼Œå‘å‰å›æº¯æŒ‡å®šå¤©æ•°

    Args:
        days_limit: å›æº¯å¤©æ•°ï¼Œé»˜è®¤ 180 å¤©ï¼ˆåŠå¹´ï¼‰
    """
    print(f"\n{'='*60}")
    print(f"ğŸš€ å¼€å§‹æŠ“å– orderFilledEvents")
    print(f"â° è¿è¡Œæ—¶é—´: {RUNTIME_TIMESTAMP}")
    print(f"ğŸ“… å›æº¯èŒƒå›´: æœ€è¿‘ {days_limit} å¤©")
    print(f"{'='*60}")
    try:
        scrape(days_limit=days_limit)
        print(f"\nâœ… orderFilledEvents æŠ“å–å®Œæˆ")
    except Exception as e:
        print(f"\nâŒ orderFilledEvents æŠ“å–é”™è¯¯: {str(e)}")